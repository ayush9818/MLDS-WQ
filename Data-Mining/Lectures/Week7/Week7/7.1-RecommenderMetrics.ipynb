{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e908bdbc-d6c9-44de-a79a-214b8192545f",
   "metadata": {},
   "source": [
    "# Recommender Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeeaf5f0-d18a-48f7-9abe-0c674e9efbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa773380-6b1b-44a7-a03d-25971a627615",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "Precision proportion of recommended items that are relevant to the user.\n",
    "\n",
    "$$Precision = \\text{Number of relevant items recommended} / \\text{Total recommended items}$$\n",
    "\n",
    "Recall measures the proportion of relevant items that are successfully recommended. \n",
    "\n",
    "$$Recall = \\text{Number of relevant items recommended} / \\text{Total relevant items}$$\n",
    "    \n",
    "These metrics are particularly useful for binary recommendation tasks, such as recommending whether a user will like, click on, purchase an item. However, they can be adapted for other types of recommendation tasks as well. \n",
    "\n",
    "For example, in the case of rating predictions (explicit feedback), precision and recall can be used by considering thresholds for predicted ratings. For instance, you can define positive interactions as ratings above a certain threshold and negative interactions as ratings below that threshold. Then, precision and recall can be calculated based on whether the predicted ratings exceed the threshold or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0027e3b6-c82a-451d-9c8a-1df94811aeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6666666666666666\n",
      "Recall: 0.6666666666666666\n",
      "F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "def precision(actual, predicted):\n",
    "    # Compute precision\n",
    "    true_positives = sum((a == 1 and p == 1) for a, p in zip(actual, predicted))\n",
    "    predicted_positives = sum(predicted)\n",
    "    return true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "\n",
    "def recall(actual, predicted):\n",
    "    # Compute recall\n",
    "    true_positives = sum((a == 1 and p == 1) for a, p in zip(actual, predicted))\n",
    "    actual_positives = sum(actual)\n",
    "    return true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "def f1_score(actual, predicted):\n",
    "    # Compute F1 score\n",
    "    prec = precision(actual, predicted)\n",
    "    rec = recall(actual, predicted)\n",
    "    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n",
    "\n",
    "# Example usage:\n",
    "actual = [1, 0, 1, 1, 0]\n",
    "predicted = [1, 1, 0, 1, 0]\n",
    "print(\"Precision:\", precision(actual, predicted))\n",
    "print(\"Recall:\", recall(actual, predicted))\n",
    "print(\"F1 Score:\", f1_score(actual, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473f6fd-387f-4e5d-a663-5a20ec13f99e",
   "metadata": {},
   "source": [
    "### RMSE and MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6111138-5e12-4160-a11c-8107232ed194",
   "metadata": {},
   "source": [
    "For recommendation systems with explicit ratings, we can calculate RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3076485e-016e-4702-af8e-6370f9726779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.3435112807463534\n",
      "MAE: 0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "def rmse(actual, predicted):\n",
    "    # Compute RMSE\n",
    "    return np.sqrt(np.mean((np.array(actual) - np.array(predicted))**2))\n",
    "\n",
    "def mae(actual, predicted):\n",
    "    # Compute MAE\n",
    "    return np.mean(np.abs(np.array(actual) - np.array(predicted)))\n",
    "\n",
    "# Example usage:\n",
    "actual_ratings = [4, 3, 5, 2, 1]\n",
    "predicted_ratings = [3.5, 2.8, 4.5, 2.1, 1.2]\n",
    "print(\"RMSE:\", rmse(actual_ratings, predicted_ratings))\n",
    "print(\"MAE:\", mae(actual_ratings, predicted_ratings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62664fb3-eb93-4744-a287-401c04ed07d0",
   "metadata": {},
   "source": [
    "### Coverage and Hit Rate\n",
    "\n",
    "Coverage is often used to measure the diversity of recommendations, while hit rate evaluates the accuracy of the recommendations in terms of user interactions. \n",
    "\n",
    "- Coverage calculates the proportion of unique items in the recommendation list over the total number of items in the catalog.\n",
    "- Hit_rate calculates the proportion of correctly recommended items (hits) out of all the items that were actually interacted with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc968139-9442-4944-8d96-7a9013d6cc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.005\n",
      "Hit Rate: 0.2\n"
     ]
    }
   ],
   "source": [
    "def coverage(recommendations, total_items):\n",
    "    # Compute coverage\n",
    "    recommended_items = set(recommendations)\n",
    "    return len(recommended_items) / total_items\n",
    "\n",
    "def hit_rate(actual, predicted):\n",
    "    # Compute hit rate\n",
    "    hits = sum((a == 1 and p == 1) for a, p in zip(actual, predicted))\n",
    "    return hits / len(actual)\n",
    "\n",
    "# Example usage:\n",
    "total_items = 1000  # Total number of items in the catalog\n",
    "recommendations = [1, 2, 3, 4, 5]  # Recommended items for a user\n",
    "actual_interactions = [1, 0, 1, 0, 0]  # Actual interactions for the user (binary)\n",
    "predicted_interactions = [1, 1, 0, 0, 0]  # Predicted interactions (binary)\n",
    "\n",
    "cov = coverage(recommendations, total_items)\n",
    "print(\"Coverage:\", cov)\n",
    "\n",
    "hit = hit_rate(actual_interactions, predicted_interactions)\n",
    "print(\"Hit Rate:\", hit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
